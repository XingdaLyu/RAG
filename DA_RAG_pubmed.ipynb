{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M-uXJB4qKCC2",
        "outputId": "3a26c836-f98c-413f-9d10-14f0707b5737"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'PRAG'...\n",
            "remote: Enumerating objects: 282, done.\u001b[K\n",
            "remote: Counting objects: 100% (282/282), done.\u001b[K\n",
            "remote: Compressing objects: 100% (210/210), done.\u001b[K\n",
            "remote: Total 282 (delta 86), reused 245 (delta 60), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (282/282), 42.92 MiB | 12.93 MiB/s, done.\n",
            "Resolving deltas: 100% (86/86), done.\n"
          ]
        }
      ],
      "source": [
        "# ! git clone https://github.com/oneal2000/PRAG.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HrND8E3oY-he",
        "outputId": "71324f2a-ddbd-4308-c88b-4003a8f08300"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JsFuXx5RKIzk",
        "outputId": "38210503-ea75-4ce5-d06d-b42939e018a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ],
      "source": [
        "# %cd /content\n",
        "# ! mv PRAG drive/MyDrive/PRAG3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pe96MNCSY72F",
        "outputId": "1cd0b4c5-3737-4f75-ad3b-cb4b0cf07b82"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/PRAG3\n",
            "all_prompt.md  configs\t\tdata_da_aug  prep_elastic.py  requirements.txt\n",
            "assets\t       data_aug.tar.gz\toutput\t     README.md\t      src\n"
          ]
        }
      ],
      "source": [
        "%cd /content/drive/MyDrive/PRAG3\n",
        "! ls"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ! tar -xzvf data_aug.tar.gz"
      ],
      "metadata": {
        "id": "ocb_HLYOOH-8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "0gUrIIvsNlkD",
        "outputId": "083ca4ae-1ce3-4b30-ebd8-984c8e2e00f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch==1.13.1 (from -r requirements.txt (line 1))\n",
            "  Downloading torch-1.13.1-cp311-cp311-manylinux1_x86_64.whl.metadata (24 kB)\n",
            "Collecting transformers==4.44.2 (from -r requirements.txt (line 2))\n",
            "  Downloading transformers-4.44.2-py3-none-any.whl.metadata (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting elasticsearch==8.15.0 (from -r requirements.txt (line 3))\n",
            "  Downloading elasticsearch-8.15.0-py3-none-any.whl.metadata (8.7 kB)\n",
            "Collecting peft==0.13.2 (from -r requirements.txt (line 4))\n",
            "  Downloading peft-0.13.2-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting pandas==1.5.3 (from -r requirements.txt (line 5))\n",
            "  Downloading pandas-1.5.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Collecting numpy==1.26.4 (from -r requirements.txt (line 6))\n",
            "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting faiss-cpu==1.8.0 (from -r requirements.txt (line 7))\n",
            "  Downloading faiss_cpu-1.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 8)) (3.0.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from torch==1.13.1->-r requirements.txt (line 1)) (4.13.2)\n",
            "Collecting nvidia-cuda-runtime-cu11==11.7.99 (from torch==1.13.1->-r requirements.txt (line 1))\n",
            "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu11==8.5.0.96 (from torch==1.13.1->-r requirements.txt (line 1))\n",
            "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu11==11.10.3.66 (from torch==1.13.1->-r requirements.txt (line 1))\n",
            "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch==1.13.1->-r requirements.txt (line 1))\n",
            "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.44.2->-r requirements.txt (line 2)) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.11/dist-packages (from transformers==4.44.2->-r requirements.txt (line 2)) (0.30.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.44.2->-r requirements.txt (line 2)) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.44.2->-r requirements.txt (line 2)) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.44.2->-r requirements.txt (line 2)) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.44.2->-r requirements.txt (line 2)) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.44.2->-r requirements.txt (line 2)) (0.5.3)\n",
            "Collecting tokenizers<0.20,>=0.19 (from transformers==4.44.2->-r requirements.txt (line 2))\n",
            "  Downloading tokenizers-0.19.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.44.2->-r requirements.txt (line 2)) (4.67.1)\n",
            "Collecting elastic-transport<9,>=8.13 (from elasticsearch==8.15.0->-r requirements.txt (line 3))\n",
            "  Downloading elastic_transport-8.17.1-py3-none-any.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from peft==0.13.2->-r requirements.txt (line 4)) (5.9.5)\n",
            "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from peft==0.13.2->-r requirements.txt (line 4)) (1.5.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.11/dist-packages (from pandas==1.5.3->-r requirements.txt (line 5)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas==1.5.3->-r requirements.txt (line 5)) (2025.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.1->-r requirements.txt (line 1)) (75.2.0)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.11/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.1->-r requirements.txt (line 1)) (0.45.1)\n",
            "INFO: pip is looking at multiple versions of accelerate to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting accelerate>=0.21.0 (from peft==0.13.2->-r requirements.txt (line 4))\n",
            "  Downloading accelerate-1.6.0-py3-none-any.whl.metadata (19 kB)\n",
            "  Downloading accelerate-1.5.1-py3-none-any.whl.metadata (19 kB)\n",
            "  Downloading accelerate-1.5.0-py3-none-any.whl.metadata (19 kB)\n",
            "  Downloading accelerate-1.4.0-py3-none-any.whl.metadata (19 kB)\n",
            "  Downloading accelerate-1.3.0-py3-none-any.whl.metadata (19 kB)\n",
            "  Downloading accelerate-1.2.1-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: urllib3<3,>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from elastic-transport<9,>=8.13->elasticsearch==8.15.0->-r requirements.txt (line 3)) (2.3.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from elastic-transport<9,>=8.13->elasticsearch==8.15.0->-r requirements.txt (line 3)) (2025.1.31)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.44.2->-r requirements.txt (line 2)) (2025.3.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.1->pandas==1.5.3->-r requirements.txt (line 5)) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.44.2->-r requirements.txt (line 2)) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.44.2->-r requirements.txt (line 2)) (3.10)\n",
            "Downloading torch-1.13.1-cp311-cp311-manylinux1_x86_64.whl (887.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m887.4/887.4 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading transformers-4.44.2-py3-none-any.whl (9.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m127.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading elasticsearch-8.15.0-py3-none-any.whl (523 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m523.3/523.3 kB\u001b[0m \u001b[31m43.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading peft-0.13.2-py3-none-any.whl (320 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.7/320.7 kB\u001b[0m \u001b[31m32.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pandas-1.5.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m137.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m88.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading faiss_cpu-1.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (27.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.0/27.0 MB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m114.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m60.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading accelerate-1.2.1-py3-none-any.whl (336 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m336.4/336.4 kB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading elastic_transport-8.17.1-py3-none-any.whl (64 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.0/65.0 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.19.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m114.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cublas-cu11, numpy, elastic-transport, pandas, nvidia-cudnn-cu11, faiss-cpu, elasticsearch, torch, tokenizers, transformers, accelerate, peft\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 2.2.2\n",
            "    Uninstalling pandas-2.2.2:\n",
            "      Successfully uninstalled pandas-2.2.2\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.6.0+cu124\n",
            "    Uninstalling torch-2.6.0+cu124:\n",
            "      Successfully uninstalled torch-2.6.0+cu124\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.21.1\n",
            "    Uninstalling tokenizers-0.21.1:\n",
            "      Successfully uninstalled tokenizers-0.21.1\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.51.3\n",
            "    Uninstalling transformers-4.51.3:\n",
            "      Successfully uninstalled transformers-4.51.3\n",
            "  Attempting uninstall: accelerate\n",
            "    Found existing installation: accelerate 1.5.2\n",
            "    Uninstalling accelerate-1.5.2:\n",
            "      Successfully uninstalled accelerate-1.5.2\n",
            "  Attempting uninstall: peft\n",
            "    Found existing installation: peft 0.14.0\n",
            "    Uninstalling peft-0.14.0:\n",
            "      Successfully uninstalled peft-0.14.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 1.5.3 which is incompatible.\n",
            "torchvision 0.21.0+cu124 requires torch==2.6.0, but you have torch 1.13.1 which is incompatible.\n",
            "dask-expr 1.1.21 requires pandas>=2, but you have pandas 1.5.3 which is incompatible.\n",
            "plotnine 0.14.5 requires pandas>=2.2.0, but you have pandas 1.5.3 which is incompatible.\n",
            "torchaudio 2.6.0+cu124 requires torch==2.6.0, but you have torch 1.13.1 which is incompatible.\n",
            "dask-cudf-cu12 25.2.2 requires pandas<2.2.4dev0,>=2.0, but you have pandas 1.5.3 which is incompatible.\n",
            "cudf-cu12 25.2.1 requires pandas<2.2.4dev0,>=2.0, but you have pandas 1.5.3 which is incompatible.\n",
            "mizani 0.13.3 requires pandas>=2.2.0, but you have pandas 1.5.3 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
            "xarray 2025.1.2 requires pandas>=2.1, but you have pandas 1.5.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed accelerate-1.2.1 elastic-transport-8.17.1 elasticsearch-8.15.0 faiss-cpu-1.8.0 numpy-1.26.4 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 pandas-1.5.3 peft-0.13.2 tokenizers-0.19.1 torch-1.13.1 transformers-4.44.2\n"
          ]
        }
      ],
      "source": [
        "! pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "STRKz-JyK4Xc",
        "outputId": "c2f14223-3244-4001-a3c5-70b2a6e69942"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "    To log in, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Enter your token (input will not be visible): \n",
            "Add token as git credential? (Y/n) n\n",
            "Token is valid (permission: fineGrained).\n",
            "The token `Colab` has been saved to /root/.cache/huggingface/stored_tokens\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful.\n",
            "The current active token is: `Colab`\n"
          ]
        }
      ],
      "source": [
        "!huggingface-cli login\n",
        "# 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"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "HcNuzp9rLaZr"
      },
      "outputs": [],
      "source": [
        "! echo 'ROOT_DIR = \"/content/drive/MyDrive/PRAG3\"' > /content/drive/MyDrive/PRAG3/src/root_dir_path.py"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! sed -i '14cDATA_ROOT_DIR = os.path.join(ROOT_DIR, \"data_da_aug\")' src/utils.py"
      ],
      "metadata": {
        "id": "ap9SCF_BxcZ_"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nvtp0yvBOIqW"
      },
      "source": [
        "# DA-RAG (final_decision Yes, No, Maybe) without COT"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! sed -i '58c\\            question = data[\"QUESTION\"]' src/inference.py\n",
        "! sed -i '60c\\            answer = data[\"final_decision\"]' src/inference.py"
      ],
      "metadata": {
        "id": "UviaH6fiPS92"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! cat src/inference.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "ScXrEsdePeep",
        "outputId": "9316e79f-2333-469c-de72-4aa7c97b0e0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "import os\n",
            "import gc\n",
            "import json\n",
            "import argparse\n",
            "import torch\n",
            "from tqdm import tqdm\n",
            "from peft import PeftModel\n",
            "\n",
            "import prompt_template\n",
            "from root_dir_path import ROOT_DIR\n",
            "from utils import get_model, evaluate, predict, load_data, read_complete\n",
            "\n",
            "def main(args):\n",
            "    data_list = load_data(args.dataset, args.data_type, args.augment_model)\n",
            "    model, tokenizer, generation_config = get_model(\n",
            "        args.model_name,\n",
            "        max_new_tokens = args.max_new_tokens,\n",
            "    )\n",
            "    if args.with_cot:\n",
            "        prompt_template.get_fewshot(args.dataset)\n",
            "    \n",
            "    cot_name = \"cot\" if args.with_cot else \"direct\"\n",
            "    load_adapter_path = os.path.join(\n",
            "        ROOT_DIR, \n",
            "        \"offline\", \n",
            "        args.model_name, \n",
            "        f\"rank={args.lora_rank}_alpha={args.lora_alpha}\",\n",
            "        args.dataset,\n",
            "        f\"lr={args.learning_rate}_epoch={args.num_train_epochs}_{cot_name}\",\n",
            "        f\"aug_model={args.augment_model}\",\n",
            "    )\n",
            "    output_root_dir = os.path.join(\n",
            "        ROOT_DIR, \n",
            "        \"output\",\n",
            "        args.model_name, \n",
            "        f\"rank={args.lora_rank}_alpha={args.lora_alpha}\",\n",
            "        args.dataset,\n",
            "        f\"lr={args.learning_rate}_epoch={args.num_train_epochs}_{cot_name}\",\n",
            "        f\"aug_model={args.augment_model}\",\n",
            "        args.inference_method, \n",
            "    )\n",
            "    for filename, fulldata in data_list:\n",
            "        filename = filename.split(\".\")[0]\n",
            "        print(f\"### Solving {filename} ###\")\n",
            "        output_dir = os.path.join(output_root_dir, filename)\n",
            "        os.makedirs(output_dir, exist_ok=True)\n",
            "        with open(os.path.join(output_dir, \"config.json\"), \"w\") as fout:\n",
            "            json.dump(vars(args), fout, indent=4)\n",
            "\n",
            "        predict_file = os.path.join(output_dir, \"predict.json\")\n",
            "        ret, start_with = read_complete(predict_file)\n",
            "\n",
            "        fulldata = fulldata[start_with:] if args.sample == -1 else fulldata[start_with:args.sample]\n",
            "        for test_id, data in tqdm(enumerate(fulldata), total=len(fulldata)):\n",
            "            test_id = test_id + start_with\n",
            "            assert test_id == len(ret), f\"test_id {test_id} != len(ret) {len(ret)}\"\n",
            "\n",
            "            question = data[\"QUESTION\"]\n",
            "            passages = data[\"passages\"]\n",
            "            answer = data[\"final_decision\"]\n",
            "\n",
            "            def get_pred(model, psgs):\n",
            "                text = predict(model, tokenizer, generation_config, \n",
            "                                        question, with_cot=args.with_cot, \n",
            "                                        passages=psgs)\n",
            "                pred = {\n",
            "                    \"test_id\": test_id, \n",
            "                    \"question\": question, \n",
            "                    \"answer\": answer, \n",
            "                    \"text\": text,\n",
            "                }\n",
            "                pred.update(evaluate(text, answer, args.with_cot))\n",
            "                return pred\n",
            "\n",
            "            if args.inference_method == \"icl\":\n",
            "                ret.append(get_pred(model, psgs=passages))\n",
            "            else:\n",
            "                for pid in range(len(passages)):\n",
            "                    adapter_path = os.path.join(load_adapter_path, filename, f\"data_{test_id}\", f\"passage_{pid}\")\n",
            "                    if pid == 0:\n",
            "                        model = PeftModel.from_pretrained(\n",
            "                            model, \n",
            "                            adapter_path,\n",
            "                            adapter_name = \"0\", \n",
            "                            is_trainable = False\n",
            "                        )\n",
            "                    else:\n",
            "                        model.load_adapter(adapter_path, adapter_name = str(pid)) \n",
            "                # merge\n",
            "                model.add_weighted_adapter(\n",
            "                    adapters = [str(i) for i in range(len(passages))], \n",
            "                    weights = [1] * len(passages),\n",
            "                    adapter_name = \"merge\", \n",
            "                    combination_type = \"cat\",\n",
            "                )\n",
            "                model.set_adapter(\"merge\")\n",
            "                ret.append(get_pred(model, psgs=None if args.inference_method == \"prag\" else passages))\n",
            "                model.delete_adapter(\"merge\")\n",
            "                model = model.unload()\n",
            "                torch.cuda.empty_cache()\n",
            "                gc.collect()\n",
            "\n",
            "        with open(predict_file, \"w\") as fout:\n",
            "            json.dump(ret, fout, indent=4)\n",
            "\n",
            "        ##### Evaluating #####\n",
            "        metrics = [\"em\", \"f1\", \"prec\", \"recall\"]\n",
            "        ret_str = \"\"\n",
            "        for met in metrics:\n",
            "            acc = sum(float(d[met]) for d in ret) / len(ret)\n",
            "            acc = round(acc, 4)\n",
            "            ret_str += f\"{met}\\t{acc}\\n\"\n",
            "        ret_str += \"\\n\" + json.dumps(vars(args), indent=4)\n",
            "        with open(os.path.join(output_dir, \"result.txt\"), \"w\") as fout:\n",
            "            fout.write(ret_str)\n",
            "\n",
            "\n",
            "if __name__ == \"__main__\":\n",
            "    parser = argparse.ArgumentParser()\n",
            "    parser.add_argument(\"--model_name\", type=str, required=True)\n",
            "    parser.add_argument(\"--max_new_tokens\", type=int, required=True)\n",
            "    parser.add_argument(\"--dataset\", type=str, required=True)\n",
            "    parser.add_argument(\"--data_type\", type=str)\n",
            "    parser.add_argument(\"--with_cot\", action=\"store_true\")\n",
            "    parser.add_argument(\"--sample\", type=int, default=-1) # -1 means all\n",
            "    parser.add_argument(\"--augment_model\", type=str, default=None)  \n",
            "    parser.add_argument(\"--num_train_epochs\", type=int, required=True)\n",
            "    parser.add_argument(\"--learning_rate\", type=float, default=3e-4)\n",
            "    parser.add_argument(\"--inference_method\", type=str, required=True, choices=[\"icl\", \"prag\", \"combine\"])\n",
            "    # LoRA\n",
            "    parser.add_argument(\"--lora_rank\", type=int)\n",
            "    parser.add_argument(\"--lora_alpha\", type=int)\n",
            "    args = parser.parse_args()\n",
            "    assert args.lora_rank and args.lora_alpha, \"No Config for LoRA\"\n",
            "    if args.augment_model is None:\n",
            "        args.augment_model = args.model_name\n",
            "    print(args)\n",
            "    main(args)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sLbJDhvWJd76",
        "outputId": "aaeed275-29d2-4422-e9ea-d722f3adaa07"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Namespace(model_name='llama3.2-1b-instruct', max_new_tokens=128, dataset='pubmedqaval', data_type=None, with_cot=True, sample=-1, augment_model='llama3.2-1b-instruct', num_train_epochs=1, learning_rate=0.0003, inference_method='icl', lora_rank=2, lora_alpha=2)\n",
            "### Solving pub_med_qa_val_merge_105 ###\n",
            "  0% 0/105 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:572: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
            "  warnings.warn(\n",
            "100% 105/105 [10:00<00:00,  5.71s/it]\n",
            "### Solving pub_med_qa_val_merge_1 ###\n",
            "100% 1/1 [00:05<00:00,  5.87s/it]\n"
          ]
        }
      ],
      "source": [
        "! python src/inference.py \\\n",
        "    --model_name llama3.2-1b-instruct \\\n",
        "    --max_new_tokens 128 \\\n",
        "    --dataset pubmedqaval \\\n",
        "    --num_train_epochs 1 \\\n",
        "    --inference_method icl \\\n",
        "    --lora_rank 2 \\\n",
        "    --lora_alpha 2 \\\n",
        "    --augment_model llama3.2-1b-instruct"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! head -n 4 /content/drive/MyDrive/PRAG3/output/llama3.2-1b-instruct/rank=2_alpha=2/pubmedqaval/lr=0.0003_epoch=1_direct/aug_model=llama3.2-1b-instruct/icl/pub_med_qa_val_merge_105/result.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ihEqB3Gb-_FH",
        "outputId": "54a6c33c-2f72-4978-cf9f-37be648837ad"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "em\t0.8857\n",
            "f1\t0.8857\n",
            "prec\t0.8857\n",
            "recall\t0.8857\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! mv /content/drive/MyDrive/PRAG3/output/llama3.2-1b-instruct/rank=2_alpha=2/pubmedqaval/lr=0.0003_epoch=1_direct/aug_model=llama3.2-1b-instruct/icl/pub_med_qa_val_merge_105_final_decision/ /content/drive/MyDrive/PRAG3/output/llama3.2-1b-instruct/rank=2_alpha=2/pubmedqaval/lr=0.0003_epoch=1_direct/aug_model=llama3.2-1b-instruct/icl/pub_med_qa_val_merge_105/"
      ],
      "metadata": {
        "id": "iEPEi0IMUfyd"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DA-RAG (final_decision Yes, No, Maybe) with COT"
      ],
      "metadata": {
        "id": "780_Vr0AY-yf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! python src/inference.py \\\n",
        "    --model_name llama3.2-1b-instruct \\\n",
        "    --max_new_tokens 128 \\\n",
        "    --dataset pubmedqaval \\\n",
        "    --num_train_epochs 1 \\\n",
        "    --inference_method icl \\\n",
        "    --lora_rank 2 \\\n",
        "    --lora_alpha 2 \\\n",
        "    --with_cot \\\n",
        "    --augment_model llama3.2-1b-instruct"
      ],
      "metadata": {
        "id": "TEy66lM7ZJGb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! head -n 4 /content/drive/MyDrive/PRAG3/output/llama3.2-1b-instruct/rank=2_alpha=2/pubmedqaval/lr=0.0003_epoch=1_cot/aug_model=llama3.2-1b-instruct/icl/pub_med_qa_val_merge_105/result.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ODvmBTA3YnDc",
        "outputId": "eea7f2f8-99a9-43ab-fbcc-445d1a7f2bfe"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "em\t0.8762\n",
            "f1\t0.8762\n",
            "prec\t0.8762\n",
            "recall\t0.8762\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}