{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M-uXJB4qKCC2",
        "outputId": "655e3d31-d790-42fd-a587-8479f302b300"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'PRAG'...\n",
            "remote: Enumerating objects: 272, done.\u001b[K\n",
            "remote: Counting objects: 100% (272/272), done.\u001b[K\n",
            "remote: Compressing objects: 100% (200/200), done.\u001b[K\n",
            "remote: Total 272 (delta 81), reused 245 (delta 60), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (272/272), 33.23 MiB | 22.61 MiB/s, done.\n",
            "Resolving deltas: 100% (81/81), done.\n"
          ]
        }
      ],
      "source": [
        "! git clone https://github.com/oneal2000/PRAG.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HrND8E3oY-he",
        "outputId": "68150397-3b94-4602-b3b9-0313edf7088b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JsFuXx5RKIzk",
        "outputId": "917d7552-957b-44b0-b344-286686247b96"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content\n"
          ]
        }
      ],
      "source": [
        "%cd /content\n",
        "! mv PRAG drive/MyDrive/PRAG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pe96MNCSY72F",
        "outputId": "b6d02b27-5c44-43c0-bb3a-6e24c62504b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/PRAG\n",
            "all_prompt.md  data_aug\t\toffline\t\t README.md\n",
            "assets\t       data_aug.tar.gz\toutput\t\t requirements.txt\n",
            "configs        data_std_aug\tprep_elastic.py  src\n"
          ]
        }
      ],
      "source": [
        "%cd /content/drive/MyDrive/PRAG\n",
        "! ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "0gUrIIvsNlkD",
        "outputId": "4a3cc40b-7ef0-49f7-e7a3-00ee9c19c764"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch==1.13.1 (from -r requirements.txt (line 1))\n",
            "  Downloading torch-1.13.1-cp311-cp311-manylinux1_x86_64.whl.metadata (24 kB)\n",
            "Collecting transformers==4.44.2 (from -r requirements.txt (line 2))\n",
            "  Downloading transformers-4.44.2-py3-none-any.whl.metadata (43 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/43.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting elasticsearch==8.15.0 (from -r requirements.txt (line 3))\n",
            "  Downloading elasticsearch-8.15.0-py3-none-any.whl.metadata (8.7 kB)\n",
            "Collecting peft==0.13.2 (from -r requirements.txt (line 4))\n",
            "  Downloading peft-0.13.2-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting pandas==1.5.3 (from -r requirements.txt (line 5))\n",
            "  Downloading pandas-1.5.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Collecting numpy==1.26.4 (from -r requirements.txt (line 6))\n",
            "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting faiss-cpu==1.8.0 (from -r requirements.txt (line 7))\n",
            "  Downloading faiss_cpu-1.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 8)) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from torch==1.13.1->-r requirements.txt (line 1)) (4.14.0)\n",
            "Collecting nvidia-cuda-runtime-cu11==11.7.99 (from torch==1.13.1->-r requirements.txt (line 1))\n",
            "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu11==8.5.0.96 (from torch==1.13.1->-r requirements.txt (line 1))\n",
            "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu11==11.10.3.66 (from torch==1.13.1->-r requirements.txt (line 1))\n",
            "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch==1.13.1->-r requirements.txt (line 1))\n",
            "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.44.2->-r requirements.txt (line 2)) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.11/dist-packages (from transformers==4.44.2->-r requirements.txt (line 2)) (0.32.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.44.2->-r requirements.txt (line 2)) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.44.2->-r requirements.txt (line 2)) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.44.2->-r requirements.txt (line 2)) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.44.2->-r requirements.txt (line 2)) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.44.2->-r requirements.txt (line 2)) (0.5.3)\n",
            "Collecting tokenizers<0.20,>=0.19 (from transformers==4.44.2->-r requirements.txt (line 2))\n",
            "  Downloading tokenizers-0.19.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.44.2->-r requirements.txt (line 2)) (4.67.1)\n",
            "Collecting elastic-transport<9,>=8.13 (from elasticsearch==8.15.0->-r requirements.txt (line 3))\n",
            "  Downloading elastic_transport-8.17.1-py3-none-any.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from peft==0.13.2->-r requirements.txt (line 4)) (5.9.5)\n",
            "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from peft==0.13.2->-r requirements.txt (line 4)) (1.7.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.11/dist-packages (from pandas==1.5.3->-r requirements.txt (line 5)) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas==1.5.3->-r requirements.txt (line 5)) (2025.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.1->-r requirements.txt (line 1)) (75.2.0)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.11/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.1->-r requirements.txt (line 1)) (0.45.1)\n",
            "INFO: pip is looking at multiple versions of accelerate to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting accelerate>=0.21.0 (from peft==0.13.2->-r requirements.txt (line 4))\n",
            "  Downloading accelerate-1.6.0-py3-none-any.whl.metadata (19 kB)\n",
            "  Downloading accelerate-1.5.2-py3-none-any.whl.metadata (19 kB)\n",
            "  Downloading accelerate-1.5.1-py3-none-any.whl.metadata (19 kB)\n",
            "  Downloading accelerate-1.5.0-py3-none-any.whl.metadata (19 kB)\n",
            "  Downloading accelerate-1.4.0-py3-none-any.whl.metadata (19 kB)\n",
            "  Downloading accelerate-1.3.0-py3-none-any.whl.metadata (19 kB)\n",
            "  Downloading accelerate-1.2.1-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: urllib3<3,>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from elastic-transport<9,>=8.13->elasticsearch==8.15.0->-r requirements.txt (line 3)) (2.4.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from elastic-transport<9,>=8.13->elasticsearch==8.15.0->-r requirements.txt (line 3)) (2025.4.26)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.44.2->-r requirements.txt (line 2)) (2025.3.2)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.44.2->-r requirements.txt (line 2)) (1.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.1->pandas==1.5.3->-r requirements.txt (line 5)) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.44.2->-r requirements.txt (line 2)) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.44.2->-r requirements.txt (line 2)) (3.10)\n",
            "Downloading torch-1.13.1-cp311-cp311-manylinux1_x86_64.whl (887.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m887.4/887.4 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading transformers-4.44.2-py3-none-any.whl (9.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m129.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading elasticsearch-8.15.0-py3-none-any.whl (523 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m523.3/523.3 kB\u001b[0m \u001b[31m42.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading peft-0.13.2-py3-none-any.whl (320 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.7/320.7 kB\u001b[0m \u001b[31m30.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pandas-1.5.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m119.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m115.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading faiss_cpu-1.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (27.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.0/27.0 MB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m112.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m52.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading accelerate-1.2.1-py3-none-any.whl (336 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m336.4/336.4 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading elastic_transport-8.17.1-py3-none-any.whl (64 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.0/65.0 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.19.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m71.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cublas-cu11, numpy, elastic-transport, pandas, nvidia-cudnn-cu11, faiss-cpu, elasticsearch, torch, tokenizers, transformers, accelerate, peft\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 2.2.2\n",
            "    Uninstalling pandas-2.2.2:\n",
            "      Successfully uninstalled pandas-2.2.2\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.6.0+cu124\n",
            "    Uninstalling torch-2.6.0+cu124:\n",
            "      Successfully uninstalled torch-2.6.0+cu124\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.21.1\n",
            "    Uninstalling tokenizers-0.21.1:\n",
            "      Successfully uninstalled tokenizers-0.21.1\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.52.4\n",
            "    Uninstalling transformers-4.52.4:\n",
            "      Successfully uninstalled transformers-4.52.4\n",
            "  Attempting uninstall: accelerate\n",
            "    Found existing installation: accelerate 1.7.0\n",
            "    Uninstalling accelerate-1.7.0:\n",
            "      Successfully uninstalled accelerate-1.7.0\n",
            "  Attempting uninstall: peft\n",
            "    Found existing installation: peft 0.15.2\n",
            "    Uninstalling peft-0.15.2:\n",
            "      Successfully uninstalled peft-0.15.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 1.5.3 which is incompatible.\n",
            "xarray 2025.3.1 requires pandas>=2.1, but you have pandas 1.5.3 which is incompatible.\n",
            "torchaudio 2.6.0+cu124 requires torch==2.6.0, but you have torch 1.13.1 which is incompatible.\n",
            "torchvision 0.21.0+cu124 requires torch==2.6.0, but you have torch 1.13.1 which is incompatible.\n",
            "plotnine 0.14.5 requires pandas>=2.2.0, but you have pandas 1.5.3 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
            "cudf-cu12 25.2.1 requires pandas<2.2.4dev0,>=2.0, but you have pandas 1.5.3 which is incompatible.\n",
            "torchdata 0.11.0 requires torch>=2, but you have torch 1.13.1 which is incompatible.\n",
            "dask-expr 1.1.21 requires pandas>=2, but you have pandas 1.5.3 which is incompatible.\n",
            "mizani 0.13.5 requires pandas>=2.2.0, but you have pandas 1.5.3 which is incompatible.\n",
            "dask-cudf-cu12 25.2.2 requires pandas<2.2.4dev0,>=2.0, but you have pandas 1.5.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed accelerate-1.2.1 elastic-transport-8.17.1 elasticsearch-8.15.0 faiss-cpu-1.8.0 numpy-1.26.4 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 pandas-1.5.3 peft-0.13.2 tokenizers-0.19.1 torch-1.13.1 transformers-4.44.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "9583a30bf1af480b9fa945892631abfd"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "! tar -xzvf data_aug.tar.gz\n",
        "! pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "STRKz-JyK4Xc",
        "outputId": "0a036aa8-4bbe-4269-b414-b916f3eb09e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "    To log in, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Enter your token (input will not be visible): \n",
            "Add token as git credential? (Y/n) n\n",
            "Token is valid (permission: fineGrained).\n",
            "The token `Colab` has been saved to /root/.cache/huggingface/stored_tokens\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful.\n",
            "The current active token is: `Colab`\n"
          ]
        }
      ],
      "source": [
        "!huggingface-cli login\n",
        "# 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"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "HcNuzp9rLaZr"
      },
      "outputs": [],
      "source": [
        "! echo 'ROOT_DIR = \"/content/drive/MyDrive/PRAG\"' > /content/drive/MyDrive/PRAG/src/root_dir_path.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "-B1kcXuXaRHl"
      },
      "outputs": [],
      "source": [
        "! sed -i '14cDATA_ROOT_DIR = os.path.join(ROOT_DIR, \"data_std_aug\")' src/utils.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "33rk8kryaI1Y",
        "outputId": "5078003a-ba59-4311-9621-ab918696e6f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "import os\n",
            "import re\n",
            "import json\n",
            "import torch\n",
            "import string\n",
            "import numpy as np\n",
            "from collections import Counter\n",
            "from typing import List, Union\n",
            "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
            "\n",
            "from root_dir_path import ROOT_DIR\n",
            "from prompt_template import get_prompt\n",
            "\n",
            "DATA_ROOT_DIR = os.path.join(ROOT_DIR, \"data_std_aug\")\n",
            "\n",
            "class BaseDataset:\n",
            "    @classmethod\n",
            "    def normalize_answer(cls, s):\n",
            "        def remove_articles(text):\n",
            "            return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n"
          ]
        }
      ],
      "source": [
        "! head -n 20 src/utils.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nvtp0yvBOIqW"
      },
      "source": [
        "# Standard-RAG (Total)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kuMJ88B_ap04",
        "outputId": "f2564b53-cee6-422d-90ef-313bd3749345"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "all_prompt.md  data_aug\t\toffline\t\t README.md\n",
            "assets\t       data_aug.tar.gz\toutput\t\t requirements.txt\n",
            "configs        data_std_aug\tprep_elastic.py  src\n"
          ]
        }
      ],
      "source": [
        "! ls /content/drive/MyDrive/PRAG"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! sed -i '58c\\            question = data[\"QUESTION\"]' src/inference.py\n",
        "! sed -i '60c\\            answer = data[\"final_decision\"]' src/inference.py"
      ],
      "metadata": {
        "id": "3hviBohunH23"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sLbJDhvWJd76",
        "outputId": "cc154c45-1baf-4dda-9913-7829de5d2199"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Namespace(model_name='llama3.2-1b-instruct', max_new_tokens=128, dataset='pubmedqaval', data_type=None, with_cot=False, sample=-1, augment_model='llama3.2-1b-instruct', num_train_epochs=1, learning_rate=0.0003, inference_method='icl', lora_rank=1, lora_alpha=1)\n",
            "### Solving pub_med_qa_val_merge_105 ###\n",
            "  0% 0/105 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:572: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
            "  warnings.warn(\n",
            "100% 105/105 [06:47<00:00,  3.88s/it]\n",
            "### Solving pub_med_qa_val_merge_1 ###\n",
            "100% 1/1 [00:04<00:00,  4.04s/it]\n"
          ]
        }
      ],
      "source": [
        "! python src/inference.py \\\n",
        "    --model_name llama3.2-1b-instruct \\\n",
        "    --max_new_tokens 128 \\\n",
        "    --dataset pubmedqaval \\\n",
        "    --num_train_epochs 1 \\\n",
        "    --inference_method icl \\\n",
        "    --lora_rank 1 \\\n",
        "    --lora_alpha 1 \\\n",
        "    --augment_model llama3.2-1b-instruct"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! echo \"total\"\n",
        "! head -n 4 /content/drive/MyDrive/PRAG/output/llama3.2-1b-instruct/rank=1_alpha=1/pubmedqaval/lr=0.0003_epoch=1_direct/aug_model=llama3.2-1b-instruct/icl/pub_med_qa_val_merge_105/result.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dJ3ycXSQ-HHQ",
        "outputId": "cead7631-ebf1-438d-bb4d-a0a7328e939d"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total\n",
            "em\t0.8286\n",
            "f1\t0.8286\n",
            "prec\t0.8286\n",
            "recall\t0.8286\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lS-DrsrEpDbG"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}